{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Comparison: Random Forest vs. Neural Network\n",
        "\n",
        "Dieses Notebook vergleicht die beiden trainierten Modelle zur Weinqualitäts-Vorhersage:\n",
        "\n",
        "**Inhalt:**\n",
        "1. Metriken-Tabelle (R², RMSE, MAE, Overfitting-Indikator)\n",
        "2. Side-by-Side Visualisierungen\n",
        "3. Ensemble-Vorhersage Test\n",
        "4. Error Analyse\n",
        "5. Statistische Tests\n",
        "\n",
        "**Voraussetzungen:**\n",
        "- Ausgeführtes `Random_Forest_GridSearch.ipynb`\n",
        "- Ausgeführtes `NN_Model.ipynb`\n",
        "- Gespeicherte Modell-Summaries in `model_history/` und `NN_model_history/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS UND KONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch import nn\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Pfade\n",
        "DATA_PATH = Path('Base-Data/winequality-red.csv')\n",
        "RF_HISTORY_DIR = Path('model_history')\n",
        "NN_HISTORY_DIR = Path('NN_model_history')\n",
        "\n",
        "# Konfiguration\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "OUTLIER_THRESHOLD = 3.0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MODEL COMPARISON NOTEBOOK\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Random Forest History: {RF_HISTORY_DIR}\")\n",
        "print(f\"Neural Network History: {NN_HISTORY_DIR}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODELL-SUMMARIES LADEN\n",
        "# ============================================================================\n",
        "\n",
        "def load_latest_summary(history_dir, pattern='*.csv', exclude_patterns=['combined', 'history', 'analysis']):\n",
        "    \"\"\"Lädt die neueste Summary-Datei aus einem History-Verzeichnis\"\"\"\n",
        "    csv_files = list(history_dir.glob(pattern))\n",
        "    # Filtere unerwünschte Dateien\n",
        "    csv_files = [f for f in csv_files if not any(excl in f.name for excl in exclude_patterns)]\n",
        "    \n",
        "    if not csv_files:\n",
        "        return None, None\n",
        "    \n",
        "    # Sortiere nach Änderungsdatum (neueste zuerst)\n",
        "    csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
        "    latest_file = csv_files[0]\n",
        "    \n",
        "    return pd.read_csv(latest_file), latest_file.name\n",
        "\n",
        "# Random Forest Summary laden\n",
        "rf_summary, rf_filename = load_latest_summary(RF_HISTORY_DIR)\n",
        "print(\"=\" * 70)\n",
        "print(\"RANDOM FOREST SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "if rf_summary is not None:\n",
        "    print(f\"Datei: {rf_filename}\")\n",
        "    print(f\"Timestamp: {rf_summary['timestamp'].values[0]}\")\n",
        "    print(f\"Test R²: {rf_summary['test_r2'].values[0]:.4f}\")\n",
        "    print(f\"Test RMSE: {rf_summary['test_rmse'].values[0]:.4f}\")\n",
        "else:\n",
        "    print(\"Keine Random Forest Summary gefunden!\")\n",
        "\n",
        "# Neural Network Summary laden\n",
        "nn_summary, nn_filename = load_latest_summary(NN_HISTORY_DIR, pattern='*_summary.csv', exclude_patterns=['history'])\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"NEURAL NETWORK SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "if nn_summary is not None:\n",
        "    print(f\"Datei: {nn_filename}\")\n",
        "    print(f\"Timestamp: {nn_summary['timestamp'].values[0]}\")\n",
        "    print(f\"Test R²: {nn_summary['test_r2'].values[0]:.4f}\")\n",
        "    print(f\"Test RMSE: {nn_summary['test_rmse'].values[0]:.4f}\")\n",
        "else:\n",
        "    print(\"Keine Neural Network Summary gefunden!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# METRIKEN-VERGLEICHSTABELLE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"METRIKEN-VERGLEICHSTABELLE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Erstelle Vergleichstabelle\n",
        "comparison_data = {\n",
        "    'Metrik': ['Test R²', 'Test RMSE', 'Test MAE', 'Train R²', 'Train RMSE', \n",
        "               'Train-Test R² Gap', 'Training Zeit (s)', 'Test Accuracy (gerundet)'],\n",
        "    'Random Forest': [\n",
        "        rf_summary['test_r2'].values[0] if rf_summary is not None else np.nan,\n",
        "        rf_summary['test_rmse'].values[0] if rf_summary is not None else np.nan,\n",
        "        rf_summary['test_mae'].values[0] if rf_summary is not None else np.nan,\n",
        "        rf_summary['train_r2'].values[0] if rf_summary is not None else np.nan,\n",
        "        rf_summary['train_rmse'].values[0] if rf_summary is not None else np.nan,\n",
        "        rf_summary['train_test_r2_diff'].values[0] if rf_summary is not None and 'train_test_r2_diff' in rf_summary.columns else np.nan,\n",
        "        rf_summary['training_time_seconds'].values[0] if rf_summary is not None else np.nan,\n",
        "        rf_summary['test_accuracy_rounded'].values[0] if rf_summary is not None and 'test_accuracy_rounded' in rf_summary.columns else np.nan,\n",
        "    ],\n",
        "    'Neural Network': [\n",
        "        nn_summary['test_r2'].values[0] if nn_summary is not None else np.nan,\n",
        "        nn_summary['test_rmse'].values[0] if nn_summary is not None else np.nan,\n",
        "        nn_summary['test_mae'].values[0] if nn_summary is not None else np.nan,\n",
        "        nn_summary['train_r2'].values[0] if nn_summary is not None else np.nan,\n",
        "        nn_summary['train_rmse'].values[0] if nn_summary is not None else np.nan,\n",
        "        abs(nn_summary['train_r2'].values[0] - nn_summary['test_r2'].values[0]) if nn_summary is not None else np.nan,\n",
        "        nn_summary['training_time_seconds'].values[0] if nn_summary is not None else np.nan,\n",
        "        nn_summary['test_acc'].values[0] if nn_summary is not None else np.nan,\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df['Gewinner'] = comparison_df.apply(\n",
        "    lambda row: 'RF' if row['Random Forest'] > row['Neural Network'] else 'NN' \n",
        "    if row['Metrik'] in ['Test R²', 'Train R²', 'Test Accuracy (gerundet)'] else\n",
        "    'RF' if row['Random Forest'] < row['Neural Network'] else 'NN',\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Markiere Gewinner mit *\n",
        "display(comparison_df.round(4))\n",
        "\n",
        "# Zusammenfassung\n",
        "rf_wins = (comparison_df['Gewinner'] == 'RF').sum()\n",
        "nn_wins = (comparison_df['Gewinner'] == 'NN').sum()\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"ZUSAMMENFASSUNG:\")\n",
        "print(f\"  Random Forest gewinnt: {rf_wins} Metriken\")\n",
        "print(f\"  Neural Network gewinnt: {nn_wins} Metriken\")\n",
        "print(f\"{'='*40}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATEN VORBEREITEN (gleiche Vorverarbeitung wie in den Modellen)\n",
        "# ============================================================================\n",
        "\n",
        "# Daten laden und bereinigen\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "initial_shape = df.shape[0]\n",
        "\n",
        "# Duplikate entfernen\n",
        "df_clean = df.drop_duplicates()\n",
        "\n",
        "# Ausreißer entfernen\n",
        "feature_cols = [c for c in df_clean.columns if c != 'quality']\n",
        "z_scores = np.abs(stats.zscore(df_clean[feature_cols]))\n",
        "outlier_mask = (z_scores < OUTLIER_THRESHOLD).all(axis=1)\n",
        "df_final = df_clean[outlier_mask].copy()\n",
        "\n",
        "print(f\"Datensatz: {initial_shape} → {df_final.shape[0]} Samples\")\n",
        "\n",
        "# Features und Target\n",
        "X = df_final.drop('quality', axis=1)\n",
        "y = df_final['quality']\n",
        "\n",
        "# Train-Test Split (gleich wie in den Modellen)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# Scaling für NN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODELLE TRAINIEREN (für direkten Vergleich)\n",
        "# ============================================================================\n",
        "\n",
        "# Random Forest (mit optimierten Parametern)\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=8,\n",
        "    min_samples_split=12,\n",
        "    min_samples_leaf=5,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=True,\n",
        "    oob_score=True,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred_train = rf_model.predict(X_train)\n",
        "rf_pred_test = rf_model.predict(X_test)\n",
        "\n",
        "# Neural Network (einfache Version ohne Training für schnellen Vergleich)\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, in_features=11):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(32, 1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "nn_model = SimpleNN(in_features=X_train.shape[1]).to(device)\n",
        "\n",
        "# Schnelles Training\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1)).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "\n",
        "print(\"Training Neural Network für Vergleich...\")\n",
        "nn_model.train()\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = nn_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 25 == 0:\n",
        "        print(f\"  Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "nn_model.eval()\n",
        "with torch.no_grad():\n",
        "    nn_pred_train = nn_model(X_train_tensor).cpu().numpy().flatten()\n",
        "    nn_pred_test = nn_model(X_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "print(\"\\nModelle trainiert!\")\n",
        "print(f\"RF Test R²: {r2_score(y_test, rf_pred_test):.4f}\")\n",
        "print(f\"NN Test R²: {r2_score(y_test, nn_pred_test):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ENSEMBLE-VORHERSAGE TESTEN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ENSEMBLE-VORHERSAGE TESTEN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verschiedene Gewichtungen testen\n",
        "weights = [\n",
        "    (0.5, 0.5, \"50/50\"),\n",
        "    (0.6, 0.4, \"60/40 (RF)\"),\n",
        "    (0.4, 0.6, \"40/60 (NN)\"),\n",
        "    (0.7, 0.3, \"70/30 (RF)\"),\n",
        "    (0.3, 0.7, \"30/70 (NN)\"),\n",
        "]\n",
        "\n",
        "ensemble_results = []\n",
        "for rf_weight, nn_weight, name in weights:\n",
        "    ensemble_pred = rf_weight * rf_pred_test + nn_weight * nn_pred_test\n",
        "    r2 = r2_score(y_test, ensemble_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))\n",
        "    mae = mean_absolute_error(y_test, ensemble_pred)\n",
        "    \n",
        "    ensemble_results.append({\n",
        "        'Gewichtung': name,\n",
        "        'RF_Weight': rf_weight,\n",
        "        'NN_Weight': nn_weight,\n",
        "        'R²': r2,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae\n",
        "    })\n",
        "\n",
        "# Einzelmodell-Ergebnisse hinzufügen\n",
        "ensemble_results.append({\n",
        "    'Gewichtung': 'RF Only',\n",
        "    'RF_Weight': 1.0,\n",
        "    'NN_Weight': 0.0,\n",
        "    'R²': r2_score(y_test, rf_pred_test),\n",
        "    'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred_test)),\n",
        "    'MAE': mean_absolute_error(y_test, rf_pred_test)\n",
        "})\n",
        "ensemble_results.append({\n",
        "    'Gewichtung': 'NN Only',\n",
        "    'RF_Weight': 0.0,\n",
        "    'NN_Weight': 1.0,\n",
        "    'R²': r2_score(y_test, nn_pred_test),\n",
        "    'RMSE': np.sqrt(mean_squared_error(y_test, nn_pred_test)),\n",
        "    'MAE': mean_absolute_error(y_test, nn_pred_test)\n",
        "})\n",
        "\n",
        "ensemble_df = pd.DataFrame(ensemble_results).sort_values('R²', ascending=False)\n",
        "display(ensemble_df.round(4))\n",
        "\n",
        "# Beste Kombination\n",
        "best_ensemble = ensemble_df.iloc[0]\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"BESTE KOMBINATION: {best_ensemble['Gewichtung']}\")\n",
        "print(f\"R²: {best_ensemble['R²']:.4f}\")\n",
        "print(f\"RMSE: {best_ensemble['RMSE']:.4f}\")\n",
        "print(f\"{'='*40}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SIDE-BY-SIDE VISUALISIERUNGEN\n",
        "# ============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Predictions vs Actual - RF\n",
        "axes[0, 0].scatter(y_test, rf_pred_test, alpha=0.5, color='steelblue', s=40)\n",
        "axes[0, 0].plot([3, 8], [3, 8], 'k--', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Tatsächliche Quality')\n",
        "axes[0, 0].set_ylabel('Vorhergesagte Quality')\n",
        "axes[0, 0].set_title(f'Random Forest (R²={r2_score(y_test, rf_pred_test):.4f})', fontweight='bold')\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# 2. Predictions vs Actual - NN\n",
        "axes[0, 1].scatter(y_test, nn_pred_test, alpha=0.5, color='crimson', s=40)\n",
        "axes[0, 1].plot([3, 8], [3, 8], 'k--', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Tatsächliche Quality')\n",
        "axes[0, 1].set_ylabel('Vorhergesagte Quality')\n",
        "axes[0, 1].set_title(f'Neural Network (R²={r2_score(y_test, nn_pred_test):.4f})', fontweight='bold')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# 3. Predictions vs Actual - Ensemble (50/50)\n",
        "ensemble_pred_50 = 0.5 * rf_pred_test + 0.5 * nn_pred_test\n",
        "axes[0, 2].scatter(y_test, ensemble_pred_50, alpha=0.5, color='green', s=40)\n",
        "axes[0, 2].plot([3, 8], [3, 8], 'k--', linewidth=2)\n",
        "axes[0, 2].set_xlabel('Tatsächliche Quality')\n",
        "axes[0, 2].set_ylabel('Vorhergesagte Quality')\n",
        "axes[0, 2].set_title(f'Ensemble 50/50 (R²={r2_score(y_test, ensemble_pred_50):.4f})', fontweight='bold')\n",
        "axes[0, 2].grid(alpha=0.3)\n",
        "\n",
        "# 4. Residual Plot - RF\n",
        "rf_residuals = y_test - rf_pred_test\n",
        "axes[1, 0].scatter(rf_pred_test, rf_residuals, alpha=0.5, color='steelblue', s=40)\n",
        "axes[1, 0].axhline(y=0, color='k', linestyle='--', linewidth=2)\n",
        "axes[1, 0].set_xlabel('Vorhergesagte Quality')\n",
        "axes[1, 0].set_ylabel('Residuen')\n",
        "axes[1, 0].set_title('RF Residual Plot', fontweight='bold')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# 5. Residual Plot - NN\n",
        "nn_residuals = y_test.values - nn_pred_test\n",
        "axes[1, 1].scatter(nn_pred_test, nn_residuals, alpha=0.5, color='crimson', s=40)\n",
        "axes[1, 1].axhline(y=0, color='k', linestyle='--', linewidth=2)\n",
        "axes[1, 1].set_xlabel('Vorhergesagte Quality')\n",
        "axes[1, 1].set_ylabel('Residuen')\n",
        "axes[1, 1].set_title('NN Residual Plot', fontweight='bold')\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "# 6. Fehlerverteilung Vergleich\n",
        "axes[1, 2].hist(rf_residuals, bins=25, alpha=0.5, label='RF', color='steelblue', density=True)\n",
        "axes[1, 2].hist(nn_residuals, bins=25, alpha=0.5, label='NN', color='crimson', density=True)\n",
        "axes[1, 2].axvline(x=0, color='k', linestyle='--', linewidth=2)\n",
        "axes[1, 2].set_xlabel('Residuen')\n",
        "axes[1, 2].set_ylabel('Dichte')\n",
        "axes[1, 2].set_title('Fehlerverteilung Vergleich', fontweight='bold')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('Model Comparison: Side-by-Side Visualisierungen', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ERROR ANALYSE: Wo liegen beide Modelle falsch?\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ERROR ANALYSE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Absolute Fehler berechnen\n",
        "rf_abs_error = np.abs(y_test.values - rf_pred_test)\n",
        "nn_abs_error = np.abs(y_test.values - nn_pred_test)\n",
        "\n",
        "# Samples wo beide stark falsch liegen (Fehler > 1)\n",
        "both_wrong = (rf_abs_error > 1) & (nn_abs_error > 1)\n",
        "rf_only_wrong = (rf_abs_error > 1) & (nn_abs_error <= 1)\n",
        "nn_only_wrong = (rf_abs_error <= 1) & (nn_abs_error > 1)\n",
        "both_correct = (rf_abs_error <= 0.5) & (nn_abs_error <= 0.5)\n",
        "\n",
        "print(f\"\\nFehleranalyse (Threshold: Fehler > 1):\")\n",
        "print(f\"  Beide stark falsch: {both_wrong.sum()} Samples ({100*both_wrong.mean():.1f}%)\")\n",
        "print(f\"  Nur RF stark falsch: {rf_only_wrong.sum()} Samples ({100*rf_only_wrong.mean():.1f}%)\")\n",
        "print(f\"  Nur NN stark falsch: {nn_only_wrong.sum()} Samples ({100*nn_only_wrong.mean():.1f}%)\")\n",
        "print(f\"  Beide gut (Fehler <= 0.5): {both_correct.sum()} Samples ({100*both_correct.mean():.1f}%)\")\n",
        "\n",
        "# Analyse der Samples wo beide falsch liegen\n",
        "if both_wrong.sum() > 0:\n",
        "    error_analysis = X_test[both_wrong].copy()\n",
        "    error_analysis['y_true'] = y_test.values[both_wrong]\n",
        "    error_analysis['rf_pred'] = rf_pred_test[both_wrong]\n",
        "    error_analysis['nn_pred'] = nn_pred_test[both_wrong]\n",
        "    error_analysis['rf_error'] = rf_abs_error[both_wrong]\n",
        "    error_analysis['nn_error'] = nn_abs_error[both_wrong]\n",
        "    \n",
        "    print(f\"\\n\\nMerkmale der schwierigen Samples (beide Modelle > 1 Fehler):\")\n",
        "    print(\"-\" * 50)\n",
        "    difficult_stats = error_analysis[['y_true', 'rf_error', 'nn_error']].describe()\n",
        "    display(difficult_stats.round(2))\n",
        "    \n",
        "    # Quality-Verteilung der schwierigen Samples\n",
        "    print(f\"\\nQuality-Verteilung der schwierigen Samples:\")\n",
        "    print(error_analysis['y_true'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STATISTISCHE TESTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STATISTISCHE TESTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Paired t-test: Sind die Unterschiede signifikant?\n",
        "# Vergleiche die quadrierten Fehler (MSE-Beitrag pro Sample)\n",
        "rf_squared_errors = (y_test.values - rf_pred_test) ** 2\n",
        "nn_squared_errors = (y_test.values - nn_pred_test) ** 2\n",
        "\n",
        "t_stat, p_value = stats.ttest_rel(rf_squared_errors, nn_squared_errors)\n",
        "\n",
        "print(f\"\\n1. PAIRED T-TEST (quadrierte Fehler)\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"   H0: Kein signifikanter Unterschied zwischen RF und NN\")\n",
        "print(f\"   t-Statistik: {t_stat:.4f}\")\n",
        "print(f\"   p-Wert: {p_value:.6f}\")\n",
        "print(f\"   Interpretation: {'Signifikanter Unterschied (p < 0.05)' if p_value < 0.05 else 'Kein signifikanter Unterschied'}\")\n",
        "\n",
        "# Wilcoxon signed-rank test (nicht-parametrisch)\n",
        "w_stat, w_p_value = stats.wilcoxon(rf_squared_errors, nn_squared_errors)\n",
        "\n",
        "print(f\"\\n2. WILCOXON SIGNED-RANK TEST\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"   H0: Kein signifikanter Unterschied zwischen RF und NN\")\n",
        "print(f\"   Statistik: {w_stat:.4f}\")\n",
        "print(f\"   p-Wert: {w_p_value:.6f}\")\n",
        "print(f\"   Interpretation: {'Signifikanter Unterschied (p < 0.05)' if w_p_value < 0.05 else 'Kein signifikanter Unterschied'}\")\n",
        "\n",
        "# Confidence Intervals für R² (Bootstrap)\n",
        "print(f\"\\n3. CONFIDENCE INTERVALS (Bootstrap, n=1000)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def bootstrap_r2(y_true, y_pred, n_bootstrap=1000, confidence=0.95):\n",
        "    \"\"\"Berechnet Bootstrap Confidence Interval für R²\"\"\"\n",
        "    r2_scores = []\n",
        "    n = len(y_true)\n",
        "    for _ in range(n_bootstrap):\n",
        "        indices = np.random.choice(n, size=n, replace=True)\n",
        "        r2 = r2_score(y_true[indices], y_pred[indices])\n",
        "        r2_scores.append(r2)\n",
        "    \n",
        "    lower = np.percentile(r2_scores, (1 - confidence) / 2 * 100)\n",
        "    upper = np.percentile(r2_scores, (1 + confidence) / 2 * 100)\n",
        "    return np.mean(r2_scores), lower, upper\n",
        "\n",
        "rf_r2_mean, rf_r2_lower, rf_r2_upper = bootstrap_r2(y_test.values, rf_pred_test)\n",
        "nn_r2_mean, nn_r2_lower, nn_r2_upper = bootstrap_r2(y_test.values, nn_pred_test)\n",
        "\n",
        "print(f\"   Random Forest R²: {rf_r2_mean:.4f} (95% CI: [{rf_r2_lower:.4f}, {rf_r2_upper:.4f}])\")\n",
        "print(f\"   Neural Network R²: {nn_r2_mean:.4f} (95% CI: [{nn_r2_lower:.4f}, {nn_r2_upper:.4f}])\")\n",
        "\n",
        "# Überlappen die Confidence Intervals?\n",
        "overlap = not (rf_r2_upper < nn_r2_lower or nn_r2_upper < rf_r2_lower)\n",
        "print(f\"   CI überlappen: {'Ja' if overlap else 'Nein'}\")\n",
        "print(f\"   Interpretation: {'Kein signifikanter Unterschied in R²' if overlap else 'Signifikanter Unterschied in R²'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zusammenfassung\n",
        "\n",
        "### Vergleichsergebnisse:\n",
        "\n",
        "**Random Forest:**\n",
        "- Vorteile: Schnelles Training, interpretierbar (Feature Importance), robust gegen Overfitting mit richtigen Parametern\n",
        "- Nachteile: Begrenzte Fähigkeit, komplexe nichtlineare Beziehungen zu erfassen\n",
        "\n",
        "**Neural Network:**\n",
        "- Vorteile: Kann komplexe Muster lernen, flexibel\n",
        "- Nachteile: Längere Trainingszeit, \"Black Box\", anfälliger für Overfitting\n",
        "\n",
        "**Ensemble:**\n",
        "- Die Kombination beider Modelle kann oft bessere Ergebnisse liefern als einzelne Modelle\n",
        "- Optimale Gewichtung hängt von den spezifischen Daten ab\n",
        "\n",
        "### Empfehlungen:\n",
        "1. Für Interpretierbarkeit: Random Forest verwenden\n",
        "2. Für maximale Accuracy: Ensemble-Ansatz testen\n",
        "3. Für Produktionseinsatz: Das einfachere Modell (RF) bevorzugen, wenn Performance ähnlich ist\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
